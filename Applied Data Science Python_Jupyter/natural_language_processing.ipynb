{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "Federalist Papers downloaded from Project Guttenberg, available here: http://www.gutenberg.org/ebooks/18 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def load_federalist_corpus(filename):\n",
    "    \"\"\" Load the federalist papers as a tokenized list of strings, one for each eassay\"\"\"\n",
    "    with open(filename, \"rt\") as f:\n",
    "        data = f.read()\n",
    "    papers = data.split(\"FEDERALIST\")\n",
    "    \n",
    "    # all start with \"To the people of the State of New York:\" (sometimes . instead of :)\n",
    "    # all end with PUBLIUS (or no end at all)\n",
    "    locations = [(i,[-1] + [m.end()+1 for m in re.finditer(r\"of the State of New York\", p)],\n",
    "                 [-1] + [m.start() for m in re.finditer(r\"PUBLIUS\", p)]) for i,p in enumerate(papers)]\n",
    "    papers_content = [papers[i][max(loc[1]):max(loc[2])] for i,loc in enumerate(locations)]\n",
    "\n",
    "    # discard entries that are not actually a paper\n",
    "    papers_content = [p for p in papers_content if len(p) > 0]\n",
    "\n",
    "    # replace all whitespace with a single space\n",
    "    papers_content = [re.sub(r\"\\s+\", \" \", p).lower() for p in papers_content]\n",
    "\n",
    "    # add spaces before all punctuation, so they are separate tokens\n",
    "    punctuation = set(re.findall(r\"[^\\w\\s]+\", \" \".join(papers_content))) - {\"-\",\"'\"}\n",
    "    for c in punctuation:\n",
    "        papers_content = [p.replace(c, \" \"+c+\" \") for p in papers_content]\n",
    "    papers_content = [re.sub(r\"\\s+\", \" \", p).lower().strip() for p in papers_content]\n",
    "    \n",
    "    authors = [tuple(re.findall(\"MADISON|JAY|HAMILTON\", a)) for a in papers]\n",
    "    authors = [a for a in authors if len(a) > 0]\n",
    "    \n",
    "    numbers = [re.search(r\"No\\. \\d+\", p).group(0) for p in papers if re.search(r\"No\\. \\d+\", p)]\n",
    "    \n",
    "    return papers_content, authors, numbers\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "papers, authors, numbers = load_federalist_corpus(\"pg18.txt\")\n",
    "\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words, and TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections # optional, but we found the collections.Counter object useful\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "def tfidf(docs):\n",
    "    \"\"\"\n",
    "    Create TFIDF matrix.  This function creates a TFIDF matrix from the\n",
    "    docs input.\n",
    "\n",
    "    Args:\n",
    "        docs: list of strings, where each string represents a space-separated\n",
    "              document\n",
    "    \n",
    "    Returns: tuple: (tfidf, all_words)\n",
    "        tfidf: sparse matrix (in any scipy sparse format) of size (# docs) x\n",
    "               (# total unique words), where i,j entry is TFIDF score for \n",
    "               document i and term j\n",
    "        all_words: list of strings, where the ith element indicates the word\n",
    "                   that corresponds to the ith column in the TFIDF matrix\n",
    "    \"\"\"\n",
    "    tf_docs = []\n",
    "    idfs = {}\n",
    "    edges = {}\n",
    "    N = len(docs)\n",
    "    \n",
    "    for thisDoc in docs:\n",
    "        words_thisDoc = thisDoc.split(\" \")\n",
    "        dict_word_cnts = dict(collections.Counter(words_thisDoc)) \n",
    "        if \"\" in dict_word_cnts:\n",
    "            del dict_word_cnts[\"\"]\n",
    "        tf_docs.append(dict_word_cnts)\n",
    "        \n",
    "        for key in dict_word_cnts:\n",
    "            if key in idfs:\n",
    "                idfs[key] += 1\n",
    "            else:\n",
    "                idfs[key] = 1\n",
    "    \n",
    "    all_words = []\n",
    "    index_ctr = {}\n",
    "    word_counter = 0\n",
    "    for word_idf in idfs:\n",
    "        index_ctr[word_idf] = word_counter\n",
    "        all_words.append(word_idf)\n",
    "        word_counter += 1\n",
    "        \n",
    "        edges[word_idf] = {}\n",
    "        doc_counter = 0\n",
    "        for thisDocDict in tf_docs:\n",
    "            if word_idf in thisDocDict:\n",
    "                if np.float(thisDocDict[word_idf] * np.log(N/idfs[word_idf])) != 0.0:\n",
    "                    edges[word_idf][doc_counter] = np.float64(thisDocDict[word_idf] * np.log(N/idfs[word_idf]))\n",
    "            doc_counter += 1\n",
    "    \n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    \n",
    "    for pkey in edges:\n",
    "        for ckey in edges[pkey]:\n",
    "            col.append(index_ctr[pkey])\n",
    "            row.append(ckey)\n",
    "            data.append(edges[pkey][ckey])\n",
    "\n",
    "    A = sp.coo_matrix((data, (row, col)), dtype = np.float64)\n",
    "    A = A.tocsr()\n",
    "    \n",
    "    return (A, all_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf,_ = tfidf(papers)\n",
    "X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.81093022  1.09861229  0.          1.09861229  1.09861229  0.40546511\n",
      "   1.09861229  1.09861229  1.09861229  1.09861229  0.40546511  0.40546511\n",
      "   0.          0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.40546511  0.          0.          0.          0.          0.40546511\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.40546511  0.40546511  1.09861229  1.09861229  1.09861229  1.09861229\n",
      "   0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.40546511  0.40546511  0.40546511\n",
      "   0.40546511  0.          0.          0.          0.          1.09861229]]\n",
      "['the', 'goal', 'of', 'this', 'lecture', 'is', 'to', 'explain', 'basics', 'free', 'text', 'processing', 'bag', 'words', 'model', 'one', 'such', 'approach', 'via']\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    \"the goal of this lecture is to explain the basics of free text processing \",\n",
    "    \"the bag of words model is one such approach\",\n",
    "    \"text processing via bag of words\" \n",
    "]\n",
    "\n",
    "# tfidf(data)\n",
    "X_tfidf, words = tfidf(data)\n",
    "print(X_tfidf.todense())\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(X):\n",
    "    \"\"\"\n",
    "    Return a matrix of cosine similarities.\n",
    "    \n",
    "    Args:\n",
    "        X: sparse matrix of TFIDF scores or term frequencies\n",
    "    \n",
    "    Returns:\n",
    "        M: dense numpy array of all pairwise cosine similarities.  That is, the \n",
    "           entry M[i,j], should correspond to the cosine similarity between the \n",
    "           ith and jth rows of X.\n",
    "    \"\"\"\n",
    "    X_dense = X.todense()\n",
    "    X_dense = np.squeeze(np.asarray(X_dense))\n",
    "    shape = (X_dense.shape[0],X_dense.shape[0])\n",
    "    result_nparray = np.ones(shape)\n",
    "    for i in range(len(X_dense)):\n",
    "        for j in range(len(X_dense)):\n",
    "            if i != j:\n",
    "                numerator = np.sum(X_dense[i]*X_dense[j])\n",
    "                doc_i_denom = np.sqrt(np.sum(np.square(X_dense[i])))\n",
    "                doc_j_denom = np.sqrt(np.sum(np.square(X_dense[j])))\n",
    "                result_nparray[i,j] = numerator / (doc_i_denom * doc_j_denom)\n",
    "                \n",
    "    return(result_nparray)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    def __init__(self, docs, n):\n",
    "        \"\"\"\n",
    "        Initialize an n-gram language model.\n",
    "        \n",
    "        Args:\n",
    "            docs: list of strings, where each string represents a space-separated\n",
    "                  document\n",
    "            n: integer, degree of n-gram model\n",
    "        \"\"\"\n",
    "        \n",
    "        self.counts = {}\n",
    "        self.n = n\n",
    "        self.count_sums = {}\n",
    "        \n",
    "        all_words = []\n",
    "        for d in docs:\n",
    "            word_list = d.split(\" \")\n",
    "            \n",
    "            all_words.extend(word_list)\n",
    "            \n",
    "            ngram_list = [tuple(word_list[i:i+n]) for i in range(len(word_list)-n+1)]\n",
    "            for ngrams in ngram_list:\n",
    "                n_minus1_tokens = ' '.join(ngrams[:n-1])\n",
    "                nth_token = ngrams[n-1]\n",
    "                if n_minus1_tokens in self.counts:\n",
    "                    if nth_token in self.counts[n_minus1_tokens]:\n",
    "                        self.counts[n_minus1_tokens][nth_token] += 1\n",
    "                    else:\n",
    "                        self.counts[n_minus1_tokens][nth_token] = 1\n",
    "                else:\n",
    "                    self.counts[n_minus1_tokens] = {}\n",
    "                    self.counts[n_minus1_tokens][nth_token] = 1\n",
    "                if n_minus1_tokens in self.count_sums:\n",
    "                    self.count_sums[n_minus1_tokens] += 1\n",
    "                else:\n",
    "                    self.count_sums[n_minus1_tokens] = 1\n",
    "\n",
    "        self.dictionary = set(all_words)\n",
    "        pass\n",
    "    \n",
    "    def perplexity(self, text, alpha=1e-3):\n",
    "        \"\"\"\n",
    "        Evaluate perplexity of model on some text.\n",
    "        \n",
    "        Args:\n",
    "            text: string containing space-separated words, on which to compute\n",
    "            alpha: constant to use in Laplace smoothing\n",
    "            \n",
    "        Note: for the purposes of smoothing, the dictionary size (i.e, the D term)\n",
    "        should be equal to the total number of unique words used to build the model\n",
    "        _and_ in the input text to this function.\n",
    "            \n",
    "        Returns: perplexity\n",
    "            perplexity: floating point value, perplexity of the text as evaluted\n",
    "                        under the model.\n",
    "        \"\"\"\n",
    "        exponent = 0\n",
    "        n = self.n\n",
    "        text_word_list = text.split(\" \")\n",
    "        \n",
    "        if len(text_word_list) < n:\n",
    "            return\n",
    "        \n",
    "        D = len(self.dictionary.union(set(text_word_list)))\n",
    "        ngram_text_list = [tuple(text_word_list[i:i+n]) for i in range(len(text_word_list)-n+1)]\n",
    "        size = len(ngram_text_list)\n",
    "        \n",
    "        for ngrams in ngram_text_list:\n",
    "            n_minus1_tokens = ' '.join(ngrams[:n-1])\n",
    "            nth_token = ngrams[n-1]\n",
    "            prob = 0\n",
    "            if n_minus1_tokens in self.counts:\n",
    "                if nth_token in self.counts[n_minus1_tokens]:\n",
    "                    prob = float(self.counts[n_minus1_tokens][nth_token] + alpha)/(self.count_sums[n_minus1_tokens] \n",
    "                                                                                   + (alpha*D))\n",
    "                else:\n",
    "                    prob = float(alpha)/(self.count_sums[n_minus1_tokens] + (alpha*D))\n",
    "            else:\n",
    "                prob = float(alpha)/((alpha*D))\n",
    "            exponent += -1 * np.log2(prob)\n",
    "\n",
    "        return(2**(exponent / size))\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    def sample(self, k):\n",
    "        \"\"\"\n",
    "        Generate a random sample of k words.\n",
    "        \n",
    "        Args:\n",
    "            k: integer, indicating the number of words to sample\n",
    "            \n",
    "        Returns: text\n",
    "            text: string of words generated from the model.\n",
    "        \"\"\"\n",
    "        n = self.n\n",
    "        text_list = random.choice(list(self.counts)).split(\" \")\n",
    "        while(len(text_list) < k):\n",
    "            start = ' '.join(text_list[len(text_list) - n + 1: ])\n",
    "            if start not in self.counts:\n",
    "                start = random.choice(list(self.counts))\n",
    "            \n",
    "            random_nthToken = random.choice(list(self.counts[start]))\n",
    "            text_list.append(random_nthToken)\n",
    "        text = ' '.join(text_list)\n",
    "\n",
    "        return text\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.5877243606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'what remedy can there be but one side , rests on mere general assertion , unsupported by any means be compared with the executive are comprehended in one way or other sinister motives , which signalizes the genius and fate of a right would ever be furnished by occasional assessments , at least you will , after all , as were before occasioned by obstructing the progress of things must rest on the evidence of this essential support , till the frail and tottering edifice seems ready to take effect , it be to enfeeble the union , an intimate intercourse between them a superiority of the next head of any candid and honest adversary of the assembly or senate , to sound the alarm when necessary ? what shall be passed . \" humanity and good se se , we are even dissimilar views in the necessity of laying taxes ought to prize causes . notwithstanding , if both the dangers that may threaten their american dominions from the diffusive construction of ships is also an established fact , commanded the esteem and applause of all others , that not seven but nine states would entertain the least dangerous'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "documents = ['a b c d e f .',\n",
    "            'b c e e f .']\n",
    "unkno = ['b a b d e f .']\n",
    "\n",
    "l_hamilton = [papers[i] for i in range(len(authors)) if len(authors[i]) == 1 and authors[i][0] == 'HAMILTON']\n",
    "l_jay = [papers[i] for i in range(len(authors)) if len(authors[i]) == 1 and authors[i][0] == 'JAY']\n",
    "l_madison = [papers[i] for i in range(len(authors)) if len(authors[i]) == 1 and authors[i][0] == 'MADISON']\n",
    "\n",
    "l_hamilton_model = LanguageModel(l_hamilton, 3)\n",
    "l_jay_model = LanguageModel(l_jay, 3)\n",
    "l_madison_model = LanguageModel(l_madison, 3)\n",
    "\n",
    "\n",
    "print(l_hamilton_model.perplexity(papers[0]))\n",
    "l_hamilton_model.sample(200)\n",
    "unknown = [papers[i] for i in range(len(authors)) if len(authors[i]) == 2]\n",
    "print(len(unknown))\n",
    "\n",
    "print(sum([l_hamilton_model.perplexity(thisDoc) for thisDoc in unknown]))\n",
    "print(sum([l_jay_model.perplexity(thisDoc) for thisDoc in unknown]))\n",
    "print(sum([l_madison_model.perplexity(thisDoc) for thisDoc in unknown]))\n",
    "\n",
    "# AUTOLAB_IGNORE_STOP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
