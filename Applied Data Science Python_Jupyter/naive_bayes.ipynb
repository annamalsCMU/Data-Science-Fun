{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    \"\"\" loads and processes data in the manner specified above\n",
    "    Inputs:\n",
    "        file_name (str): path to csv file containing data\n",
    "    Outputs:\n",
    "        pd.DataFrame: processed dataframe\n",
    "    \"\"\"\n",
    "    d_f = pd.read_csv(file_name, sep = ',', header = 0, na_values = '?')\n",
    "    d_f = d_f.dropna(axis = 0, how= 'any')\n",
    "    d_f['label'] = d_f['income'].replace({'>50K': 1, '<=50K':0})\n",
    "    del d_f['income']\n",
    "    d_f.reset_index(inplace=True)\n",
    "    \n",
    "    return d_f\n",
    "\n",
    "df = load_data('census.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Naive Bayes classifier\n",
    "Let $X_1, X_2, \\ldots, X_k$ be the $k$ features of a dataset, with class label given by the variable $y$. A probabilistic classifier assigns the most probable class to each instance $(x_1,\\ldots,x_k)$, as expressed by\n",
    "$$ \\hat{y} = \\arg\\max_y P(y\\ |\\ x_1,\\ldots,x_k) $$\n",
    "\n",
    "Using Bayes' theorem, the above *posterior probability* can be rewritten as\n",
    "$$ P(y\\ |\\ x_1,\\ldots,x_k) = \\frac{P(y) P(x_1,\\ldots,x_n\\ |\\ y)}{P(x_1,\\ldots,x_k)} $$\n",
    "where\n",
    "- $P(y)$ is the prior probability of the class\n",
    "- $P(x_1,\\ldots,x_k\\ |\\ y)$ is the likelihood of data under a class\n",
    "- $P(x_1,\\ldots,x_k)$ is the evidence for data\n",
    "\n",
    "Naive Bayes classifiers assume that the feature values are conditionally independent given the class label, that is,\n",
    "$ P(x_1,\\ldots,x_n\\ |\\ y) = \\prod_{i=1}^{k}P(x_i\\ |\\ y) $. This strong assumption helps simplify the expression for posterior probability to\n",
    "$$ P(y\\ |\\ x_1,\\ldots,x_k) = \\frac{P(y) \\prod_{i=1}^{k}P(x_i\\ |\\ y)}{P(x_1,\\ldots,x_k)} $$\n",
    "\n",
    "For a given input $(x_1,\\ldots,x_k)$, $P(x_1,\\ldots,x_k)$ is constant. Hence, we can simplify omit the denominator replace the equality sign with proportionality as follows:\n",
    "$$ P(y\\ |\\ x_1,\\ldots,x_k) \\propto P(y) \\prod_{i=1}^{k}P(x_i\\ |\\ y) $$\n",
    "\n",
    "Thus, the class of a new instance can be predicted as $\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{k}P(x_i\\ |\\ y)$. Here, $P(y)$ is commonly known as the **class prior** and $P(x_i\\ |\\ y)$ termed **feature predictor**. The rest of the assignment deals with how each of these $k+1$ probability distributions -- $P(y), P(x_1\\ |\\ y), \\ldots, P(x_k\\ |\\ y)$ -- are estimated from data.\n",
    "\n",
    "\n",
    "**Note**: Observe that the computation of the final expression above involve multiplication of $k+1$ probability values (which can be really low). This can lead to an underflow of numerical precision. So, it is a good practice to use a log transform of the probabilities to avoid this underflow.\n",
    "\n",
    "** TL;DR ** Final take away from this cell is the following expression:\n",
    "$$\\hat{y} = \\arg\\max_y \\underbrace{\\log P(y)}_{log-prior} + \\underbrace{\\sum_{i=1}^{k} \\log P(x_i\\ |\\ y)}_{log-likelihood}$$\n",
    "\n",
    "Each term in the sum for log-likelihood can be regarded a partial log-likelihood based on a particular feature alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Predictor\n",
    "The beauty of a Naive Bayes classifier lies in the fact we can mix-and-match different likelihood models for each feature predictor according to the prior knowledge we have about it and these models can be varied independent of each other. For example, we might know that $P(X_i|y)$ for some continuous feature $X_i$ is normally distributed or that $P(X_i|y)$ for some categorical feature follows multinomial distribution. In such cases, we can directly plugin the pdf/pmf of these distributions in place of $P(x_i\\ |\\ y)$.\n",
    "\n",
    "- Gaussian model, for continuous real-valued features (parameterized by mean $\\mu$ and variance $\\sigma$)\n",
    "- Categorical model, for discrete features (parameterized by $\\mathbf{p} = <p_0,\\ldots,p_{l-1}>$, where $l$ is the number of values taken by this categorical feature)\n",
    "\n",
    "- **Parameter estimation `init()`**: Learn parameters of the likelihood model using MLE (Maximum Likelihood Estimator). Need to keep track of $k$ sets of parameters, one for each class, *in the increasing order of class id, i.e., mu[i] indicates the mean of class $i$ in the Gaussian Predictor*.\n",
    "- **Partial Log-Likelihood computation for *this* feature `partial_log_likelihood()`**: Use the learnt parameters to compute the probability (density/mass for continuous/categorical features) of a given feature value. Report np.log() of this value.\n",
    "\n",
    "The parameter estimation is for the conditional distributions $P(X|Y)$. Thus, while estimating parameters for a specific class (say class 0), we will use only those data points in the training set (or rows in the input data frame) which have class label 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Feature Predictor\n",
    "The Guassian distribution is characterized by two parameters - mean $\\mu$ and standard deviation $\\sigma$:\n",
    "$$ f_Z(z) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp{(-\\frac{(z-\\mu)^2}{2\\sigma^2})} $$\n",
    "\n",
    "Given $n$ samples $z_1, \\ldots, z_n$ from the above distribution, the MLE for mean and standard deviation are:\n",
    "$$ \\hat{\\mu} = \\frac{1}{n} \\sum_{j=1}^{n} z_j $$\n",
    "\n",
    "$$ \\hat{\\sigma} = \\sqrt{\\frac{1}{n} \\sum_{j=1}^{n} (z_j-\\hat{\\mu})^2} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -3.63166766,  -3.2524249 ],\n",
       "       [ -3.55071473,  -3.32238449],\n",
       "       [-14.60226337, -18.13920716],\n",
       "       [ -5.47164304,  -8.71608989]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GaussianPredictor:\n",
    "    \"\"\" Feature predictor for a normally distributed real-valued, continuous feature.\n",
    "        Attributes: \n",
    "            mu (array_like) : vector containing per class mean of the feature\n",
    "            sigma (array_like): vector containing per class std. deviation of the feature\n",
    "    \"\"\"\n",
    "    # feel free to define and use any more attributes, e.g., number of classes, etc\n",
    "    def __init__(self, x, y) :\n",
    "        \"\"\" initializes the predictor statistics (mu, sigma) for Gaussian distribution\n",
    "        Inputs:\n",
    "            x (array_like): feature values (continuous)\n",
    "            y (array_like): class labels (0,...,k-1)\n",
    "        \"\"\"\n",
    "        self.k = len(np.unique(y))\n",
    "        self.mu = np.zeros(self.k)\n",
    "        self.sigma = np.zeros(self.k)\n",
    "        for i in range(self.k):\n",
    "            self.mu[i] = np.mean([x[j] for j in range(len(y)) if y[j] == i])\n",
    "            self.sigma[i] = np.std([x[j] for j in range(len(y)) if y[j] == i])\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def partial_log_likelihood(self, x):\n",
    "        \"\"\" log likelihood of feature values x according to each class\n",
    "        Inputs:\n",
    "            x (array_like): vector of feature values\n",
    "        Outputs:\n",
    "            (array_like): matrix of log likelihood for this feature alone\n",
    "        \"\"\"\n",
    "        result_mat = np.zeros((len(x), self.k))\n",
    "        for i in range(len(x)):\n",
    "            for j in range(self.k):\n",
    "                result_mat[i][j] = np.log(stats.norm.pdf(x[i], loc=self.mu[j], scale=self.sigma[j]))\n",
    "        return result_mat\n",
    "        pass\n",
    "\n",
    "f = GaussianPredictor(df['age'], df['label'])\n",
    "f.mu\n",
    "f.sigma\n",
    "f.partial_log_likelihood([43,40,100,10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Feature Predictor\n",
    "The categorical distribution with $l$ categories $\\{0,\\ldots,l-1\\}$ is characterized by parameters $\\mathbf{p} = (p_0,\\dots,p_{l-1})$:\n",
    "$$ P(z; \\mathbf{p}) = p_0^{[z=0]}p_1^{[z=1]}\\ldots p_{l-1}^{[z=l-1]} $$\n",
    "\n",
    "where $[z=t]$ is 1 if $z$ is $t$ and 0 otherwise.\n",
    "\n",
    "Given $n$ samples $z_1, \\ldots, z_n$ from the above distribution, the smoothed-MLE for each $p_t$ is:\n",
    "$$ \\hat{p_t} = \\frac{n_t + \\alpha}{n + l\\alpha} $$\n",
    "\n",
    "where $n_t = \\sum_{j=1}^{n} [z_j=t]$, i.e., the number of times the label $t$ occurred in the sample. The smoothing is done to avoid zero-count problem (similar in spirit to $n$-gram model in NLP).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.48243939, -0.16040634],\n",
       "       [-0.96044059, -1.90917639],\n",
       "       [-0.48243939, -0.16040634]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CategoricalPredictor:\n",
    "    \"\"\" Feature predictor for a categorical feature.\n",
    "        Attributes: \n",
    "            p (dict) : dictionary of vector containing per class probability of a feature value;\n",
    "                    the keys of dictionary should exactly match the values taken by this feature\n",
    "    \"\"\"\n",
    "    # feel free to define and use any more attributes, e.g., number of classes, etc\n",
    "    def __init__(self, x, y, alpha=1) :\n",
    "        \"\"\" initializes the predictor statistics (p) for Categorical distribution\n",
    "        Inputs:\n",
    "            x (array_like): feature values (categorical)\n",
    "            y (array_like): class labels (0,...,k-1)\n",
    "        \"\"\"\n",
    "        self.p = {}\n",
    "        x_features = np.unique(x)\n",
    "        y_labels = np.unique(y)\n",
    "        \n",
    "        self.p = {thisX: np.zeros(len(y_labels)) for thisX in x_features}\n",
    "        \n",
    "        for i in range(len(y_labels)):\n",
    "            sample = [x[j] for j in range(len(x)) if y[j] == y_labels[i]]\n",
    "            for thisX in x_features:\n",
    "                self.p[thisX][i] = (np.sum([1 for t in sample if t == thisX]) + alpha) / (len(sample) + (len(x_features)*alpha))\n",
    "        \n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def partial_log_likelihood(self, x):\n",
    "        \"\"\" log likelihood of feature values x according to each class\n",
    "        Inputs:\n",
    "            x (array_like): vector of feature values\n",
    "        Outputs:\n",
    "            (array_like): matrix of log likelihood for this feature\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for thisX in x:\n",
    "            thisResult = np.zeros(len(self.p[thisX]))\n",
    "            for i in range(len(thisResult)):\n",
    "                thisResult[i] = np.log(self.p[thisX][i])\n",
    "            result.append(thisResult)\n",
    "        \n",
    "        return np.array(result)\n",
    "\n",
    "f = CategoricalPredictor(df['sex'], df['label'])\n",
    "f.p\n",
    "f.partial_log_likelihood(['Male','Female','Male'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting things together\n",
    "It's time to put all the feature predictors together and do something useful! \n",
    "\n",
    "1. **__init__()**: Compute the log prior for each class and initialize the feature predictors (based on feature type). The smoothed prior for class $t$ is given by\n",
    "$$ prior(t) = \\frac{n_t + \\alpha}{n + k\\alpha} $$\n",
    "where $n_t = \\sum_{j=1}^{n} [y_j=t]$, i.e., the number of times the label $t$ occurred in the sample. \n",
    "\n",
    "2. **predict()**: For each instance and for each class, compute the sum of log prior and partial log likelihoods for all features. Use it to predict the final class label. Break ties by predicting the class with lower id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: RuntimeWarning: divide by zero encountered in log\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:35: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0. ...,  0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    \"\"\" Naive Bayes classifier for a mixture of continuous and categorical attributes.\n",
    "        We use GaussianPredictor for continuous attributes and MultinomialPredictor for categorical ones.\n",
    "        Attributes:\n",
    "            predictor (dict): model for P(X_i|Y) for each i\n",
    "            log_prior (array_like): log P(Y)\n",
    "    \"\"\"\n",
    "    # feel free to define and use any more attributes, e.g., number of classes, etc\n",
    "    def __init__(self, df, alpha=1):\n",
    "        \"\"\"initializes predictors for each feature and computes class prior\n",
    "        Inputs:\n",
    "            df (pd.DataFrame): processed dataframe, without any missing values.\n",
    "        \"\"\"\n",
    "        self.labels = np.unique(df['label'])\n",
    "        self.log_prior = np.zeros(len(self.labels))\n",
    "        for i in range(len(self.labels)):\n",
    "            self.log_prior[i] = np.log((np.sum([1 for val in df['label'] if self.labels[i] == val])+alpha)/(len(df['label'])+(len(self.labels)*alpha)))\n",
    "\n",
    "        self.predictor = {}\n",
    "        for thisCol in df:\n",
    "            if thisCol not in ['index', 'label']:\n",
    "                self.predictor[thisCol] = GaussianPredictor(df[thisCol], df['label']) if df[thisCol].dtype == 'int64' else CategoricalPredictor(df[thisCol], df['label'], alpha)\n",
    "        \n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\" predicts label for input instances from log_prior and partial_log_likelihood of feature predictors\n",
    "        Inputs:\n",
    "            x (pd.DataFrame): processed dataframe, without any missing values and without class label.\n",
    "        Outputs:\n",
    "            (array_like): array of predicted class labels (0,..,k-1)\n",
    "        \"\"\"        \n",
    "        ctr = 0\n",
    "        pred_labels = np.zeros(len(x))\n",
    "        \n",
    "        thisPred = np.zeros((len(x),len(self.labels)))\n",
    "        for i in range(len(self.labels)):\n",
    "            thisPred[:,i] += self.log_prior[self.labels[i]]\n",
    "\n",
    "        for c in x:\n",
    "            if c not in ['index', 'label']:\n",
    "                partial_ll = self.predictor[c].partial_log_likelihood(x[c])\n",
    "                for i in range(len(self.labels)):\n",
    "                    thisPred[:,i] += partial_ll[:,i]\n",
    "        \n",
    "        for this in thisPred:\n",
    "            pred_labels[ctr] = np.argmax(this)\n",
    "            ctr += 1\n",
    "\n",
    "        return pred_labels\n",
    "        \n",
    "        pass\n",
    "\n",
    "c = NaiveBayesClassifier(df, 0)\n",
    "y_pred = c.predict(df)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation - Error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17240236058616804"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(y_hat, y):\n",
    "    \"\"\" Evaluates classifier predictions\n",
    "        Inputs:\n",
    "            y_hat (array_like): output from classifier\n",
    "            y (array_like): true class label\n",
    "        Output:\n",
    "            (double): error rate as defined above\n",
    "    \"\"\"\n",
    "    y = np.array(y)\n",
    "    return np.mean(y_hat != y)\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "evaluate(y_pred, df['label'])\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
