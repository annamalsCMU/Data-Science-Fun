{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing my own graph class and Page ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup using networkx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "import networkx as nx\n",
    "Gr = nx.read_edgelist(\"wikipedia_small.graph\",create_using=nx.DiGraph())\n",
    "Gr.edges()\n",
    "nx.shortest_path_length(G, source = \"4368\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My own Graph class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement `add_edges()` call\n",
    "\n",
    "To begin, implement the function `add_edges()`.  This will modify the `self.edges` variable to add all edges passed as tuples in `edges_list`.  Note that `self.edges` should be represented as an \"adjacency dictionary\", so that for every node `i` in the graph\n",
    "\n",
    "    self.edges[i]\n",
    "    \n",
    "indicators another dictionary of nodes that `i` is connected to (with the direction pointing from `i` to the other node).  For instance, if `self.edges[i][j]` is a valid entry this means that there is an edge from node `i` to node `j` (the value of this entry doesn't matter, so we could technically use a dictionary of sets, but we use a dictionary of dictionaries to keep things a little bit more uniform and to allow for potential extensions e.g. to weighted graphs).\n",
    "\n",
    "For instance, the following code populates a simple graph like the following:\n",
    "\n",
    "    A -> B -> C\n",
    "    |    ^\n",
    "    v    |\n",
    "    D -> E\n",
    "    \n",
    "Note that all vertices must exist in the dictionary. If a vertex has no outgoing edges, then it should still have an entry pointing to an empty dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Djisktra's algorithm\n",
    "\n",
    "Next, implement Djisktra's single-source shortest path algorithm (with the simple case where the distance along any edge is assumed to be one).  We can refer to the Wikipedia page on Djikstra's algorithm for reference (https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm), but the basic idea of the algorithm is to keep a priority queue of the current known distances to a node.  We continually pop off the smallest element `i` in the queue, and then update all its successor nodes to have a distance of 1 + distance[i].\n",
    "\n",
    "For the priority queue, we should use the heapdict library (https://github.com/DanielStutzbach/heapdict) that is included above (and included with Anaconda).  We can update the priority of a heapdict element and pop off the smallest element using the syntax\n",
    "\n",
    "    d = heapdict({\"a\":5, \"b\":6})\n",
    "    d[\"b\"] = 4\n",
    "    d.popitem() # -> (\"b\", 4)\n",
    "    ...\n",
    "    \n",
    "    \n",
    "The function should return a list both of the shortest distances and the previous nodes in the shortest path from the source node to this node. \n",
    "\n",
    "* The distance for unreachable nodes should be inf\n",
    "* The path for the source node should be None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjacency matrix representation\n",
    "\n",
    "Implement the adjacency matrix method of the Graph class.  This returns a matrix representing the adjacency of the graph (in scipy COO sparse format), as well as a list of nodes that indicate how the indices in this graph relate to the nodes in the network.\n",
    "\n",
    "IMPORTANT: in order to complete this question in a manner that works on the Wikipedia, we will _need_ to implement this function natively as a sparse matrix (i.e., we cannot construct a dense matrix and then convert that to a sparse matrix, but need to directly use the `sp.coo_matrix()` constructor).  The Wikipedia graph is is 24K x 24K nodes, which (assuming 8 bytes per entry, would take up 4GB of memory.  While it's not impossible to do things this way at this scale (it quickly becomes infeasible for graphs that are even slightly larger), it's a very bad idea, and just allocating this much memory will take too long.\n",
    "\n",
    "For example, in our graph above we have the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank algorithm\n",
    "\n",
    "Finally, implement the PageRank algorithm using the adjacency matrix representation that we built in the previous question.  Reference on the PageRank algorithm is here: https://en.wikipedia.org/wiki/PageRank.  We should specifically use the approach described in the \"Power Method\" section on this page, which we also discussed in class.  This involves forming some initial uniform probability vector $x$, and repeatly multiplying it by the matrices:\n",
    "\\begin{equation}\n",
    "x := (d P + ((1-d)/n) E)x\n",
    "\\end{equation}\n",
    "where $P$ is a transition matrix (the $A$ matrix normalized so that all columns have sum 1), $E$ is the matrix of all ones, and $d$ is the damping factor.\n",
    "\n",
    "NOTES: Recall that from the definition of PageRank, when we reach a \"sink\" node (a node with no outgoing edges), we randomly hope to any other node in the network, so that columns of $P$ that have no outgoing edges are set to the uniform distribution.  To be efficient, we'll also want to avoid explicitly forming the $E$ matrix, and should instead use the fact that $E = 11^T$ where $1$ denotes a vector of all ones.  Use the fact that we can reorder matrix multiplication if associative (i.e., the fact the $A(BC)$ = $(AB)C$) to make this operation as fast as possible.\n",
    "\n",
    "Our function should return a dictionary of nodes and their corresponding page rank.  For example, in our graph above, we have the following reults:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import heapdict\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self):\n",
    "        \"\"\" Initialize with an empty edge dictionary. \"\"\"\n",
    "        self.edges = {}\n",
    "    \n",
    "    def add_edges(self, edges_list):\n",
    "        \"\"\" Add a list of edges to the network. Use 1.0 to indiciate the presence of an edge. \n",
    "        \n",
    "        Args:\n",
    "            edges_list: list of (a,b) tuples, where a->b is an edge to add\n",
    "        \"\"\"\n",
    "        for thisTuple in edges_list:\n",
    "            if thisTuple[0] in self.edges:\n",
    "                if thisTuple[1] not in self.edges[thisTuple[0]]:\n",
    "                    self.edges[thisTuple[0]][thisTuple[1]] = 1.0\n",
    "            else:\n",
    "                self.edges[thisTuple[0]] = {}\n",
    "                self.edges[thisTuple[0]][thisTuple[1]] = 1.0\n",
    "            if thisTuple[1] not in self.edges:\n",
    "                self.edges[thisTuple[1]] = {}\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    def shortest_path(self, source):\n",
    "        \"\"\" Compute the single-source shorting path.\n",
    "        \n",
    "        This function uses Djikstra's algorithm to compute the distance from \n",
    "        source to all other nodes in the network.\n",
    "        \n",
    "        Args:\n",
    "            source: node index for the source\n",
    "            \n",
    "        Returns: tuple: dist, path\n",
    "            dist: dictionary of node:distance values for each node in the graph, \n",
    "                  where distance denotes the shortest path distance from source\n",
    "            path: dictionary of node:prev_node values, where prev_node indicates\n",
    "                  the previous node on the path from source to node\n",
    "        \"\"\"\n",
    "        dist_map = {}\n",
    "        path_map = {}\n",
    "        \n",
    "        initial_node = source\n",
    "        dist_map[source] = 0\n",
    "        path_map[source] = None\n",
    "        \n",
    "        for key in self.edges:\n",
    "            if key not in dist_map:\n",
    "                dist_map[key] = float('inf')\n",
    "                path_map[key] = None\n",
    "                \n",
    "        d = heapdict.heapdict(dist_map)\n",
    "        \n",
    "        while(len(d) > 0):\n",
    "            current = d.popitem()\n",
    "\n",
    "            if current[1] == float('inf'):\n",
    "                continue\n",
    "\n",
    "            for key in self.edges[current[0]]:\n",
    "                if key in d:\n",
    "                    dist_map[key] = min(dist_map[key], dist_map[current[0]]+1)\n",
    "                    if path_map[key] == None:\n",
    "                        path_map[key] = current[0]    \n",
    "                    d[key] = dist_map[key]\n",
    "\n",
    "        return (dist_map, path_map) \n",
    "        pass\n",
    "    \n",
    "        \n",
    "    def adjacency_matrix(self):\n",
    "        \"\"\" Compute an adjacency matrix form of the graph.  \n",
    "        \n",
    "        Returns: tuple (A, nodes)\n",
    "            A: a sparse matrix in COO form that represents the adjaency matrix\n",
    "               for the graph (i.e., A[j,i] = 1 iff there is an edge i->j)\n",
    "               NOTE: be sure you have this ordering correct!\n",
    "            nodes: a list of nodes indicating the node key corresponding to each\n",
    "                   index of the A matrix\n",
    "        \"\"\"\n",
    "        index_dict = {}\n",
    "        index_ctr = 0\n",
    "        nList = []\n",
    "        for key in self.edges:\n",
    "            index_dict[key] = index_ctr\n",
    "            index_ctr += 1\n",
    "            nList.append(key)\n",
    "        \n",
    "        row = []\n",
    "        col = []\n",
    "        data = []\n",
    "        for pkey in self.edges:\n",
    "            if len(self.edges[pkey]) > 0:\n",
    "                for ckey in self.edges[pkey]:\n",
    "                    col.append(index_dict[pkey])\n",
    "                    row.append(index_dict[ckey])\n",
    "                    data.append(1)\n",
    "        \n",
    "        A = sp.coo_matrix((data, (row, col)),shape = (len(self.edges), len(self.edges)))\n",
    "\n",
    "        return (A, nList)\n",
    "\n",
    "    \n",
    "    def pagerank(self, d=0.85, iters=100):\n",
    "        \"\"\" Compute the PageRank score for each node in the network.\n",
    "        \n",
    "        Compute PageRank scores using the power method.\n",
    "        \n",
    "        Args:\n",
    "            d: 1 - random restart factor\n",
    "            iters: maximum number of iterations of power method\n",
    "            \n",
    "        Returns: dict ranks\n",
    "            ranks: a dictionary of node:importance score, for each node in the\n",
    "                   network (larger score means higher rank)\n",
    "        \n",
    "        \"\"\"\n",
    "        rank_dict = {}\n",
    "        with np.errstate(divide='ignore'):\n",
    "            M,nList = self.adjacency_matrix()\n",
    "            M_csr = M.tocsr()\n",
    "            n,_ = M_csr.shape\n",
    "\n",
    "            rank_vect = np.ones((n,1))/n\n",
    "            out_deg = M_csr.sum(axis=0)\n",
    "            out_deg_vect = out_deg.T / d\n",
    "\n",
    "            while iters > 0:\n",
    "                new_rank_vect = M_csr.dot((rank_vect/out_deg_vect))\n",
    "                new_rank_vect += (1-new_rank_vect.sum())/n\n",
    "                rank_vect = new_rank_vect\n",
    "                iters -= 1\n",
    "            rank_list = []\n",
    "            for item in rank_vect.tolist():\n",
    "                rank_list.append(item[0])\n",
    "            \n",
    "            return dict(zip(nList, rank_list))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': {'B': 1.0, 'D': 1.0}, 'B': {'C': 1.0}, 'C': {}, 'D': {'E': 1.0}, 'E': {'B': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "G = Graph()\n",
    "# G.add_edges([(\"A\",\"B\"), (\"B\",\"A\"), (\"C\", \"B\"), (\"C\",\"A\")])\n",
    "G.add_edges([(\"A\",\"B\"), (\"B\",\"C\"), (\"A\",\"D\"), (\"D\", \"E\"), (\"E\", \"B\")])\n",
    "print(G.edges)\n",
    "# G.shortest_path(\"C\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "# print(G.shortest_path('1753'))\n",
    "print(G.shortest_path(\"B\"))\n",
    "print(G.shortest_path(\"C\"))\n",
    "print(G.shortest_path(\"D\"))\n",
    "# print(G.shortest_path(\"E\"))\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1, 0)\t1\n",
      "  (3, 0)\t1\n",
      "  (2, 1)\t1\n",
      "  (4, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 4)\t1\n",
      "  (2, 1)\t1\n",
      "  (3, 0)\t1\n",
      "  (4, 3)\t1\n",
      "['A', 'B', 'C', 'D', 'E']\n"
     ]
    }
   ],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "A, nlist = G.adjacency_matrix()\n",
    "print(A)\n",
    "print(A.tocsr())\n",
    "print(nlist)\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0.0851086238706817,\n",
       " 'B': 0.2812467668696596,\n",
       " 'C': 0.3241683757098924,\n",
       " 'D': 0.12127978901572142,\n",
       " 'E': 0.1881964445340449}"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "G.pagerank()\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    United_States 0.00275188705264\n",
    "    2007 0.00244339053989\n",
    "    2008 0.00217342514773\n",
    "    Wikimedia_Commons 0.00172604200122\n",
    "    United_Kingdom 0.00159998851013\n",
    "    2006 0.00154844911674\n",
    "    France 0.00144958994072\n",
    "    Wiktionary 0.00126753476354\n",
    "    Canada 0.00109896195215\n",
    "    World_War_II 0.00104913079624\n",
    "    2005 0.00104633024568\n",
    "    List_of_African_films 0.00100713870383\n",
    "    Germany 0.000956262192248\n",
    "    Europe 0.000937690025073\n",
    "    English_language 0.000908144359626\n",
    "    Geographic_coordinate_system 0.000891711151403\n",
    "    Latin 0.000888662228804\n",
    "    Australia 0.000879854710005\n",
    "    India 0.000787625093175\n",
    "    Japan 0.000784815389935\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
