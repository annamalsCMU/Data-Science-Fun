{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Text Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process(text, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" Normalizes case and handles punctuation\n",
    "    Inputs:\n",
    "        text: str: raw text\n",
    "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "                    (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
    "    Outputs:\n",
    "        list(str): tokenized text\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = (text.replace(\"'s\", \"\")).replace(\"'\",\"\")\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    text = text.translate(translator)    \n",
    "    tokens = nltk.word_tokenize(str(text))\n",
    "\n",
    "    token_list = []\n",
    "    for word in tokens:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        token_list.append(lemma)\n",
    "        \n",
    "    return token_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'sample', 'test', 'input', 'for', 'processing']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a sample test input for processing.\"\n",
    "print(process(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      screen_name                                               text\n",
      "0             GOP  RT @GOPconvention: #Oregon votes today. That m...\n",
      "1    TheDemocrats  RT @DWStweets: The choice for 2016 is clear: W...\n",
      "2  HillaryClinton  Trump's calling for trillion dollar tax cuts f...\n",
      "3  HillaryClinton  .@TimKaine's guiding principle: the belief tha...\n",
      "4        timkaine  Glad the Senate could pass a #THUD / MilCon / ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tweets = pd.read_csv(\"tweets_train.csv\", na_filter=False)\n",
    "print(tweets.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      screen_name                                               text\n",
      "0             GOP  [rt, gopconvention, oregon, vote, today, that,...\n",
      "1    TheDemocrats  [rt, dwstweets, the, choice, for, 2016, is, cl...\n",
      "2  HillaryClinton  [trump, calling, for, trillion, dollar, tax, c...\n",
      "3  HillaryClinton  [timkaine, guiding, principle, the, belief, th...\n",
      "4        timkaine  [glad, the, senate, could, pas, a, thud, milco...\n"
     ]
    }
   ],
   "source": [
    "def process_all(df, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" process all text in the dataframe using process_text() function.\n",
    "    Inputs\n",
    "        df: pd.DataFrame: dataframe containing a column 'text' loaded from the CSV file\n",
    "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "                    (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
    "    Outputs\n",
    "        pd.DataFrame: dataframe in which the values of text column have been changed from str to list(str),\n",
    "                        the output from process_text() function. Other columns are unaffected.\n",
    "    \"\"\"\n",
    "    result_df = df.copy(deep=True)\n",
    "    counter = 0\n",
    "    for thisText in result_df.text:\n",
    "        result_df.text[counter] = process(thisText)\n",
    "        counter += 1\n",
    "        \n",
    "    return result_df\n",
    "\n",
    "\n",
    "processed_tweets = process_all(tweets)\n",
    "print(processed_tweets.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Construction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      screen_name                                               text\n",
      "0             GOP  [rt, gopconvention, oregon, vote, today, that,...\n",
      "1    TheDemocrats  [rt, dwstweets, the, choice, for, 2016, is, cl...\n",
      "2  HillaryClinton  [trump, calling, for, trillion, dollar, tax, c...\n",
      "3  HillaryClinton  [timkaine, guiding, principle, the, belief, th...\n",
      "4        timkaine  [glad, the, senate, could, pas, a, thud, milco...\n",
      "21280\n"
     ]
    }
   ],
   "source": [
    "def get_rare_words(processed_tweets):\n",
    "    \"\"\" use the word count information across all tweets in training data to come up with a feature list\n",
    "    Inputs:\n",
    "        processed_tweets: pd.DataFrame: the output of process_all() function\n",
    "    Outputs:\n",
    "        list(str): list of rare words, sorted alphabetically.\n",
    "    \"\"\"\n",
    "    list_all_words = []\n",
    "    for thisText in processed_tweets.text:\n",
    "        list_all_words.extend(thisText)\n",
    "    word_dict_ctr = dict(Counter(list_all_words))\n",
    "    rare_word_list = [word for word in word_dict_ctr if word_dict_ctr[word] == 1]\n",
    "    rare_word_list.sort()\n",
    "\n",
    "    return rare_word_list\n",
    "    \n",
    "print(processed_tweets.head())\n",
    "rare_words = get_rare_words(processed_tweets)\n",
    "print(len(rare_words)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a sparse matrix of features for each tweet with the help of `sklearn.feature_extraction.text.TfidfVectorizer`. Remember to ignore the rare words obtained above and NLTK's stop words during the feature creation step. We must leave other optional parameters (e.g., `vocab`, `norm`, etc) at their default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',...üá∫üá∏üá∫üá∏üá∫üá∏', 'üöôclean', 'üö®üö®', 'üö™', 'üö™close', 'üö´choice', 'üö´climate', 'üö´obamacare', 'üö´üë∑', 'üö¥', 'ü§ë', 'ü§îüôÑüôÖüèº'],\n",
      "        strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "def create_features(processed_tweets, rare_words):\n",
    "    \"\"\" creates the feature matrix using the processed tweet text\n",
    "    Inputs:\n",
    "        tweets: pd.DataFrame: tweets read from train/test csv file, containing the column 'text'\n",
    "        rare_words: list(str): one of the outputs of get_feature_and_rare_words() function\n",
    "    Outputs:\n",
    "        sklearn.feature_extraction.text.TfidfVectorizer: the TfidfVectorizer object used\n",
    "                                                we need this to tranform test tweets in the same way as train tweets\n",
    "        scipy.sparse.csr.csr_matrix: sparse bag-of-words TF-IDF feature matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    stop_words.extend(rare_words)\n",
    "    vect = sklearn.feature_extraction.text.TfidfVectorizer(stop_words = stop_words)\n",
    "    tweet_array = [\" \".join(thistext) for thistext in processed_tweets.text]\n",
    "\n",
    "    bow_matrix = vect.fit_transform(tweet_array)\n",
    "    \n",
    "    return(vect, bow_matrix)\n",
    "\n",
    "(tfidf, X) = create_features(processed_tweets, rare_words)\n",
    "print(tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also for each tweet, assign a class label (0 or 1) using its `screen_name`. Use 0 for realDonaldTrump, mike_pence, GOP and 1 for the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ..., 0 1 0]\n",
      "8652\n"
     ]
    }
   ],
   "source": [
    "def create_labels(processed_tweets):\n",
    "    \"\"\" creates the class labels from screen_name\n",
    "    Inputs:\n",
    "        tweets: pd.DataFrame: tweets read from train file, containing the column 'screen_name'\n",
    "    Outputs:\n",
    "        numpy.ndarray(int): dense binary numpy array of class labels\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.array([0 if name in (\"realDonaldTrump\", \"mike_pence\", \"GOP\") else 1 for name in processed_tweets.screen_name], dtype=int)\n",
    "    \n",
    "y = create_labels(processed_tweets)\n",
    "print(y)\n",
    "print(len([k for k in y if k == 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "def learn_classifier(X_train, y_train, kernel='best'):\n",
    "    \"\"\" learns a classifier from the input features and labels using the kernel function supplied\n",
    "    Inputs:\n",
    "        X_train: scipy.sparse.csr.csr_matrix: sparse matrix of features, output of create_features_and_labels()\n",
    "        y_train: numpy.ndarray(int): dense binary vector of class labels, output of create_features_and_labels()\n",
    "        kernel: str: kernel function to be used with classifier. [best|linear|poly|rbf|sigmoid]\n",
    "                    if 'best' is supplied, reset the kernel parameter to the value you have determined to be the best\n",
    "    Outputs:\n",
    "        sklearn.svm.classes.SVC: classifier learnt from data\n",
    "    \"\"\"\n",
    "    if kernel == 'best':\n",
    "        kernel = 'linear' # entered after testing which is the best kernel fuction\n",
    "        \n",
    "    svm_classifier = sklearn.svm.SVC(kernel = kernel)\n",
    "    svm_classifier.fit(X_train,y_train)\n",
    "\n",
    "    return svm_classifier\n",
    "\n",
    "classifier = learn_classifier(X, y, 'linear')\n",
    "print(classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.956700196555\n"
     ]
    }
   ],
   "source": [
    "def evaluate_classifier(classifier, X_validation, y_validation):\n",
    "    \"\"\" evaluates a classifier based on a supplied validation data\n",
    "    Inputs:\n",
    "        classifier: sklearn.svm.classes.SVC: classifer to evaluate\n",
    "        X_train: scipy.sparse.csr.csr_matrix: sparse matrix of features\n",
    "        y_train: numpy.ndarray(int): dense binary vector of class labels\n",
    "    Outputs:\n",
    "        double: accuracy of classifier on the validation data\n",
    "    \"\"\"\n",
    "    y_predict = classifier.predict(X_validation)\n",
    "    return np.sum(y_predict == y_validation)/len(y_validation)\n",
    "    \n",
    "accuracy = evaluate_classifier(classifier, X, y)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear : 0.956700196555\n",
      "rbf : 0.500173430454\n",
      "poly : 0.500173430454\n",
      "sigmoid : 0.500173430454\n"
     ]
    }
   ],
   "source": [
    "# AUTOLAB_IGNORE_START\n",
    "for kernel in ['linear', 'rbf', 'poly', 'sigmoid']:\n",
    "    classifier = learn_classifier(X, y, kernel)\n",
    "    accuracy = evaluate_classifier(classifier, X, y)\n",
    "    print(kernel,':',accuracy)\n",
    "# AUTOLAB_IGNORE_STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_tweets(tfidf, classifier, unlabeled_tweets):\n",
    "    \"\"\" predicts class labels for raw tweet text\n",
    "    Inputs:\n",
    "        tfidf: sklearn.feature_extraction.text.TfidfVectorizer: the TfidfVectorizer object used on training data\n",
    "        classifier: sklearn.svm.classes.SVC: classifier learnt\n",
    "        unlabeled_tweets: pd.DataFrame: tweets read from tweets_test.csv\n",
    "    Outputs:\n",
    "        numpy.ndarray(int): dense binary vector of class labels for unlabeled tweets\n",
    "    \"\"\"\n",
    "    \n",
    "    unlabelled_processed = process_all(unlabeled_tweets, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer())\n",
    "    tweet_array = []\n",
    "    for i in range(len(unlabelled_processed)):\n",
    "        tweet_array.append(\" \".join(thisT for thisT in unlabelled_processed.text[i]))\n",
    "\n",
    "    transform_unl_text = tfidf.transform(tweet_array)\n",
    "    predict = classifier.predict(transform_unl_text)\n",
    "    return predict\n",
    "    \n",
    "\n",
    "classifier = learn_classifier(X, y, 'best')\n",
    "unlabeled_tweets = pd.read_csv(\"tweets_test.csv\", na_filter=False)\n",
    "y_pred = classify_tweets(tfidf, classifier, unlabeled_tweets)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
